{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice neural nets and ODEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class objdict(dict):\n",
    "    \"\"\" Wrap a dict so entries act as properties like an object.\n",
    "        From https://goodcode.io/articles/python-dict-object/ .\n",
    "    \"\"\"\n",
    "    \n",
    "    def __getattr__(self, name):\n",
    "        if name in self:\n",
    "            return self[name]\n",
    "        else:\n",
    "            raise AttributeError(\"No such attribute: \" + name)\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        self[name] = value\n",
    "\n",
    "    def __delattr__(self, name):\n",
    "        if name in self:\n",
    "            del self[name]\n",
    "        else:\n",
    "            raise AttributeError(\"No such attribute: \" + name)\n",
    "\n",
    "_args = {\n",
    "    'network': 'odenet',          # 'resnet', 'odenet'\n",
    "    'tol': 1e-3,\n",
    "    'adjoint': False,\n",
    "    'downsampling-method': 'res', # conv, res\n",
    "    'nepochs': 160,\n",
    "    'data_aug': True,\n",
    "    'lr': 0.1,\n",
    "    'batch_size': 128,\n",
    "    'test_batch_size': 1000,\n",
    "    'save': 'experiment1',\n",
    "#    'debug': 'store_true',\n",
    "    'gpu': 0\n",
    "}\n",
    "\n",
    "args = objdict(_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'network': 'odenet', 'tol': 0.001, 'adjoint': False, 'downsampling-method': 'res', 'nepochs': 160, 'data_aug': True, 'lr': 0.1, 'batch_size': 128, 'test_batch_size': 1000, 'save': 'experiment1', 'gpu': 0}\n"
     ]
    }
   ],
   "source": [
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.adjoint:\n",
    "    from torchdiffeq import odeint_adjoint as odeint\n",
    "else:\n",
    "    from torchdiffeq import odeint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "def norm(dim):\n",
    "    return nn.GroupNorm(min(32, dim), dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_loaders(data_aug=False, batch_size=128, test_batch_size=1000, perc=1.0):\n",
    "    if data_aug:\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomCrop(28, padding=4),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "    else:\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        datasets.MNIST(root='./data/mnist', train=True, download=True, transform=transform_train), batch_size=batch_size,\n",
    "        shuffle=True, num_workers=2, drop_last=True\n",
    "    )\n",
    "\n",
    "    train_eval_loader = DataLoader(\n",
    "        datasets.MNIST(root='./data/mnist', train=True, download=True, transform=transform_test),\n",
    "        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        datasets.MNIST(root='./data/mnist', train=False, download=True, transform=transform_test),\n",
    "        batch_size=test_batch_size, shuffle=False, num_workers=2, drop_last=True\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader, train_eval_loader\n",
    "\n",
    "def inf_generator(iterable):\n",
    "    \"\"\"Allows training with DataLoaders in a single infinite loop:\n",
    "        for i, (x, y) in enumerate(inf_generator(train_loader)):\n",
    "    \"\"\"\n",
    "    iterator = iterable.__iter__()\n",
    "    while True:\n",
    "        try:\n",
    "            yield iterator.__next__()\n",
    "        except StopIteration:\n",
    "            iterator = iterable.__iter__()\n",
    "\n",
    "\n",
    "def learning_rate_with_decay(batch_size, batch_denom, batches_per_epoch, boundary_epochs, decay_rates):\n",
    "    initial_learning_rate = args.lr * batch_size / batch_denom\n",
    "\n",
    "    boundaries = [int(batches_per_epoch * epoch) for epoch in boundary_epochs]\n",
    "    vals = [initial_learning_rate * decay for decay in decay_rates]\n",
    "\n",
    "    def learning_rate_fn(itr):\n",
    "        lt = [itr < b for b in boundaries] + [True]\n",
    "        i = np.argmax(lt)\n",
    "        return vals[i]\n",
    "\n",
    "    return learning_rate_fn\n",
    "\n",
    "\n",
    "def one_hot(x, K):\n",
    "    return np.array(x[:, None] == np.arange(K)[None, :], dtype=int)\n",
    "\n",
    "\n",
    "def accuracy(model, dataset_loader):\n",
    "    total_correct = 0\n",
    "    for x, y in dataset_loader:\n",
    "        x = x.to(device)\n",
    "        y = one_hot(np.array(y.numpy()), 10)\n",
    "\n",
    "        target_class = np.argmax(y, axis=1)\n",
    "        predicted_class = np.argmax(model(x).cpu().detach().numpy(), axis=1)\n",
    "        total_correct += np.sum(predicted_class == target_class)\n",
    "    return total_correct / len(dataset_loader.dataset)\n",
    "\n",
    "\n",
    "def makedirs(dirname):\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)\n",
    "\n",
    "\n",
    "def get_logger(logpath, package_files=[], displaying=True, saving=True, debug=False):\n",
    "    logger = logging.getLogger()\n",
    "    if debug:\n",
    "        level = logging.DEBUG\n",
    "    else:\n",
    "        level = logging.INFO\n",
    "    logger.setLevel(level)\n",
    "    if saving:\n",
    "        info_file_handler = logging.FileHandler(logpath, mode=\"a\")\n",
    "        info_file_handler.setLevel(level)\n",
    "        logger.addHandler(info_file_handler)\n",
    "    if displaying:\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setLevel(level)\n",
    "        logger.addHandler(console_handler)\n",
    "\n",
    "    return logger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConcatConv2d for the ODENet NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatConv2d(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_in, dim_out, ksize=3, stride=1, padding=0, dilation=1, groups=1, bias=True, transpose=False):\n",
    "        super(ConcatConv2d, self).__init__()\n",
    "        module = nn.ConvTranspose2d if transpose else nn.Conv2d\n",
    "        self._layer = module(\n",
    "            dim_in + 1, dim_out, kernel_size=ksize, stride=stride, padding=padding, dilation=dilation, groups=groups,\n",
    "            bias=bias\n",
    "        )\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        tt = torch.ones_like(x[:, :1, :, :]) * t\n",
    "        ttx = torch.cat([tt, x], 1)\n",
    "        return self._layer(ttx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ODEFunc for the ODENet NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODEfunc(nn.Module):\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super(ODEfunc, self).__init__()\n",
    "        self.norm1 = norm(dim)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv1 = ConcatConv2d(dim, dim, 3, 1, 1)\n",
    "        self.norm2 = norm(dim)\n",
    "        self.conv2 = ConcatConv2d(dim, dim, 3, 1, 1)\n",
    "        self.norm3 = norm(dim)\n",
    "        self.nfe = 0\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        self.nfe += 1\n",
    "        out = self.norm1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv1(t, out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(t, out)\n",
    "        out = self.norm3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ODEBlock for the ODENet NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODEBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, odefunc):\n",
    "        super(ODEBlock, self).__init__()\n",
    "        self.odefunc = odefunc\n",
    "        self.integration_time = torch.tensor([0, 1]).float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.integration_time = self.integration_time.type_as(x)\n",
    "        out = odeint(self.odefunc, x, self.integration_time, rtol=args.tol, atol=args.tol)\n",
    "        return out[1]\n",
    "\n",
    "    @property\n",
    "    def nfe(self):\n",
    "        return self.odefunc.nfe\n",
    "\n",
    "    @nfe.setter\n",
    "    def nfe(self, value):\n",
    "        self.odefunc.nfe = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        shape = torch.prod(torch.tensor(x.shape[1:])).item()\n",
    "        return x.view(-1, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningAverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self, momentum=0.99):\n",
    "        self.momentum = momentum\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = None\n",
    "        self.avg = 0\n",
    "\n",
    "    def update(self, val):\n",
    "        if self.val is None:\n",
    "            self.avg = val\n",
    "        else:\n",
    "            self.avg = self.avg * self.momentum + val * (1 - self.momentum)\n",
    "        self.val = val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResBlock class. This is for the ResNet NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.norm1 = norm(inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.norm2 = norm(planes)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "\n",
    "        out = self.relu(self.norm1(x))\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            shortcut = self.downsample(out)\n",
    "\n",
    "        out = self.conv1(out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "\n",
    "        return out + shortcut\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_odenet = args.network == 'odenet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define device that will execute the NN\n",
    "device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsampling layers\n",
    "downsampling_layers = [\n",
    "    nn.Conv2d(1, 64, 3, 1),\n",
    "    ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\n",
    "    ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature layers\n",
    "feature_layers = [ODEBlock(ODEfunc(64))] if is_odenet else [ResBlock(64, 64) for _ in range(6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FC layers\n",
    "fc_layers = [norm(64), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1)), Flatten(), nn.Linear(64, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion function\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data loaders\n",
    "train_loader, test_loader, train_eval_loader = get_mnist_loaders(\n",
    "    args.data_aug, args.batch_size, args.test_batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data generator and batches per epoch\n",
    "data_gen = inf_generator(train_loader)\n",
    "batches_per_epoch = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get learning rate with decay\n",
    "lr_fn = learning_rate_with_decay(\n",
    "    args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],\n",
    "    decay_rates=[1, 0.1, 0.01, 0.001]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize counters\n",
    "best_acc = 0\n",
    "batch_time_meter = RunningAverageMeter()\n",
    "f_nfe_meter = RunningAverageMeter()\n",
    "b_nfe_meter = RunningAverageMeter()\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'network': 'odenet', 'tol': 0.001, 'adjoint': False, 'downsampling-method': 'res', 'nepochs': 160, 'data_aug': True, 'lr': 0.1, 'batch_size': 128, 'test_batch_size': 1000, 'save': 'experiment1', 'gpu': 0}\n"
     ]
    }
   ],
   "source": [
    "# get logger\n",
    "makedirs(args.save)\n",
    "logger = get_logger(logpath=os.path.join(args.save, 'logs'))\n",
    "logger.info(args)\n",
    "\n",
    "device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0000 | Time 6.375 (6.375) | NFE-F 26.0 | NFE-B 0.0 | Train Acc 0.0992 | Test Acc 0.1009\n",
      "Epoch 0001 | Time 4.656 (4.570) | NFE-F 20.2 | NFE-B 0.0 | Train Acc 0.9772 | Test Acc 0.9797\n",
      "Epoch 0002 | Time 4.670 (4.516) | NFE-F 20.1 | NFE-B 0.0 | Train Acc 0.9897 | Test Acc 0.9905\n",
      "Epoch 0003 | Time 4.669 (4.504) | NFE-F 20.1 | NFE-B 0.0 | Train Acc 0.9878 | Test Acc 0.9872\n",
      "Epoch 0004 | Time 4.642 (4.503) | NFE-F 20.1 | NFE-B 0.0 | Train Acc 0.9914 | Test Acc 0.9904\n",
      "Epoch 0005 | Time 4.661 (4.498) | NFE-F 20.1 | NFE-B 0.0 | Train Acc 0.9884 | Test Acc 0.9868\n",
      "Epoch 0006 | Time 4.710 (4.521) | NFE-F 20.1 | NFE-B 0.0 | Train Acc 0.9931 | Test Acc 0.9912\n",
      "Epoch 0007 | Time 4.663 (4.507) | NFE-F 20.1 | NFE-B 0.0 | Train Acc 0.9940 | Test Acc 0.9927\n",
      "Epoch 0008 | Time 4.682 (4.528) | NFE-F 20.1 | NFE-B 0.0 | Train Acc 0.9953 | Test Acc 0.9936\n",
      "Epoch 0009 | Time 4.797 (4.597) | NFE-F 20.1 | NFE-B 0.0 | Train Acc 0.9954 | Test Acc 0.9937\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-68409ba3ac1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mfeature_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnfe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/neural_odes/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/neural_odes/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# run\n",
    "for itr in range(args.nepochs * batches_per_epoch):\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr_fn(itr)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    x, y = data_gen.__next__()\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    logits = model(x)\n",
    "    loss = criterion(logits, y)\n",
    "\n",
    "    if is_odenet:\n",
    "        nfe_forward = feature_layers[0].nfe\n",
    "        feature_layers[0].nfe = 0\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if is_odenet:\n",
    "        nfe_backward = feature_layers[0].nfe\n",
    "        feature_layers[0].nfe = 0\n",
    "\n",
    "    batch_time_meter.update(time.time() - end)\n",
    "    if is_odenet:\n",
    "        f_nfe_meter.update(nfe_forward)\n",
    "        b_nfe_meter.update(nfe_backward)\n",
    "    end = time.time()\n",
    "\n",
    "    if itr % batches_per_epoch == 0:\n",
    "        with torch.no_grad():\n",
    "            train_acc = accuracy(model, train_eval_loader)\n",
    "            val_acc = accuracy(model, test_loader)\n",
    "            if val_acc > best_acc:\n",
    "                torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))\n",
    "                best_acc = val_acc\n",
    "            logger.info(\n",
    "                \"Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | \"\n",
    "                \"Train Acc {:.4f} | Test Acc {:.4f}\".format(\n",
    "                    itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,\n",
    "                    b_nfe_meter.avg, train_acc, val_acc\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
